{
    "function": "bin.qtabformerv3.main",
    "gpus": [
        "NVIDIA GeForce RTX 4090"
    ],
    "config": {
        "seed": 8,
        "batch_size": 512,
        "patience": 16,
        "n_epochs": -1,
        "amp": true,
        "data": {
            "cache": true,
            "path": "data/regression-num-medium-0-medical_charges",
            "num_policy": "noisy-quantile"
        },
        "optimizer": {
            "type": "AdamW",
            "lr": 5.9805095753224706e-05,
            "weight_decay": 2.6225852075493297e-05
        },
        "model": {
            "use_key_as_value": true,
            "encoder_n_blocks": 1,
            "distance_metric": "cossim",
            "temperature": 0.1,
            "momentum": 0.999,
            "queue_ratio": 64,
            "d_main": 64,
            "d_multiplier": 2.0,
            "mixer_normalization": "auto",
            "dropout0": 0.0,
            "dropout1": 0.0,
            "normalization": "LayerNorm",
            "activation": "ReLU",
            "num_embeddings": {
                "type": "PeriodicEmbeddings",
                "n_frequencies": 89,
                "frequency_init_scale": 0.05908651647102354,
                "d_embedding": 46,
                "lite": true
            }
        }
    },
    "n_parameters": 42614,
    "prediction_type": "labels",
    "epoch_size": 20,
    "best_step": 1500,
    "metrics": {
        "train": {
            "rmse": 0.07872768630928874,
            "mae": 0.05094234645366669,
            "r2": 0.9802109677319107,
            "score": -0.07872768630928874
        },
        "val": {
            "rmse": 0.08257981643279816,
            "mae": 0.052132464945316315,
            "r2": 0.9787620930085457,
            "score": -0.08257981643279816
        },
        "test": {
            "rmse": 0.08201216010083721,
            "mae": 0.051798202097415924,
            "r2": 0.9791033021568266,
            "score": -0.08201216010083721
        }
    },
    "time": "0:00:27.943907",
    "chunk_size": null,
    "eval_batch_size": 32768
}